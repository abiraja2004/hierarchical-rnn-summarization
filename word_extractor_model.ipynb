{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Possible improvement:\n",
    "    - pre-trained language model\n",
    "    - beam search\n",
    "    \n",
    "    TODO:\n",
    "    - build word dictionary\n",
    "    - add EOS on output sequence\n",
    "    - replace named-entity\n",
    "    - output projection, sampled softmax\n",
    "    - extract arg_max and embed, update embedding only for \"go\"\n",
    "    - decoder input for testing is only [GO, PAD, ..., PAD]\n",
    "\"\"\"\n",
    "print 'Start!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.util import nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Setup Variables and Placeholders for Convolutional Sentence Encoder\n",
    "\"\"\"\n",
    "# Store layers weight & bias\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "word_embedding_size = 150\n",
    "sentence_length = 50 # fixed length\n",
    "vocab_size = 42579\n",
    "\n",
    "filter_shape_1 = [filter_sizes[0], word_embedding_size, 1, num_filters]\n",
    "filter_shape_2 = [filter_sizes[1], word_embedding_size, 1, num_filters]\n",
    "filter_shape_3 = [filter_sizes[2], word_embedding_size, 1, num_filters]\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.truncated_normal(filter_shape_1, stddev=0.1), name='wc1'),\n",
    "    'wc2': tf.Variable(tf.truncated_normal(filter_shape_2, stddev=0.1), name='wc2'),\n",
    "    'wc3': tf.Variable(tf.truncated_normal(filter_shape_3, stddev=0.1), name='wc3'),\n",
    "    'word_embeddings': tf.Variable(tf.random_uniform([vocab_size, word_embedding_size], 1., -1.), trainable=False, name='word_embeddings_150_6_20')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([num_filters], name='bc1')),\n",
    "    'bc2': tf.Variable(tf.random_normal([num_filters], name='bc2')),\n",
    "    'bc3': tf.Variable(tf.random_normal([num_filters], name='bc3'))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_length = 45\n",
    "batch_size = 5 # size of training batch\n",
    "sentence_embedding_size = 300 # encoder input size\n",
    "doc_embedding_size = 750 # hidden layer size\n",
    "output_size = 1\n",
    "learning_rate = 1e-3\n",
    "momentum_beta_1 = 0.99\n",
    "momentum_beta_2 = 0.999\n",
    "\n",
    "linear = tf.nn.rnn_cell._linear\n",
    "\n",
    "proj_w_t = tf.Variable(tf.random_uniform([vocab_size, doc_embedding_size], minval=-0.05, maxval=0.05), name='proj_w')\n",
    "proj_w = tf.transpose(proj_w_t)\n",
    "proj_b = tf.Variable(tf.random_uniform([vocab_size], minval=-0.05, maxval=0.05), name='proj_b')\n",
    "\n",
    "variable_dict = {\n",
    "    \"encoder_cell\": tf.nn.rnn_cell.BasicLSTMCell(doc_embedding_size, state_is_tuple=True),\n",
    "    \"decoder_cell\": tf.nn.rnn_cell.BasicLSTMCell(doc_embedding_size, state_is_tuple=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "placeholders = {\n",
    "    \"sentences_input_3\": tf.placeholder(tf.int32, shape=[None, 3, sentence_length], name='input_bucket_3'),\n",
    "    \"summary_words\": tf.placeholder(tf.int32, shape=[None, summary_length + 1]),\n",
    "    \"feedfw_sampling\": tf.placeholder(tf.bool),\n",
    "    \"sampled_prob\": tf.placeholder(tf.float32, shape=[2]),\n",
    "    \"keep_prob\": tf.placeholder(tf.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_seq2seq_step(\n",
    "        enc_cell,\n",
    "        dec_cell,\n",
    "        encoder_inputs,\n",
    "        decoder_inputs,\n",
    "        output_targets=None,\n",
    "        is_sampled=True,\n",
    "        sampled_prob=0.0,\n",
    "        keep_prob=1.0,\n",
    "        dtype=tf.float32):\n",
    "    encoder_outputs, enc_state = rnn_encoder(enc_cell, encoder_inputs, dtype=dtype)\n",
    "    \n",
    "    # Compute a concatenation of encoder outputs to put attention on.\n",
    "    top_states = [tf.reshape(e, [-1, 1, enc_cell.output_size])\n",
    "                  for e in encoder_outputs]\n",
    "    attention_states = tf.concat(1, top_states)\n",
    "    \n",
    "    decoder_outputs, dec_state = embedding_rnn_attention_decoder(dec_cell, enc_state, encoder_outputs, \n",
    "                                             decoder_inputs, attention_states,\n",
    "                                             output_targets=output_targets, \n",
    "                                             is_sampled=is_sampled, sampled_prob=sampled_prob,\n",
    "                                             keep_prob=keep_prob)\n",
    "    \n",
    "    return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_encoder(cell, encoder_inputs, dtype=tf.float32):\n",
    "    outputs, state = tf.nn.rnn(cell, encoder_inputs, dtype=dtype)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO CURRICULUM LEARNING\n",
    "# WHAT HAPPEN WHEN TESTING? NO INPUTS PROVIDED TO DECODER\n",
    "# IF IS_SAMPLED inp = prev\n",
    "# decoder inputs would be just GO, PAD, ..., PAD of fixed length\n",
    "# TODO output should be softmax-ed -> MLP\n",
    "# DIFFERENT ATTENTION MECHANISM, see eq. 10 from the paper\n",
    "# Different fully connected layer architecture\n",
    "def embedding_rnn_attention_decoder(cell, \n",
    "        initial_state,   \n",
    "        encoder_states, \n",
    "        decoder_inputs,\n",
    "        attention_states,\n",
    "        output_targets=None, \n",
    "        is_sampled=True,\n",
    "        sampled_prob=0.0,\n",
    "        num_heads=1,\n",
    "        keep_prob=1.0,\n",
    "        initial_state_attention=False,\n",
    "        dtype=tf.float32\n",
    "       ):\n",
    "    \"\"\"\n",
    "        - decoder_inputs: list of 1D batch-sized int32 tensors\n",
    "    \"\"\"\n",
    "    # lookup the embeddings of the decoder inputs\n",
    "    word_embeddings = weights['word_embeddings']\n",
    "\n",
    "    decoder_embed_inputs = [tf.nn.embedding_lookup(word_embeddings, i)\n",
    "                            for i in decoder_inputs]\n",
    "    \n",
    "    batch_size = tf.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n",
    "    attn_length = attention_states.get_shape()[1].value\n",
    "    if attn_length is None:\n",
    "        attn_length = shape(attention_states)[1]\n",
    "    attn_size = attention_states.get_shape()[2].value\n",
    "\n",
    "    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n",
    "    hidden = tf.reshape(\n",
    "        attention_states, [-1, attn_length, 1, attn_size])\n",
    "    hidden_features = []\n",
    "    v = []\n",
    "    attention_vec_size = attn_size  # Size of query vectors for attention.\n",
    "    for a in xrange(num_heads):\n",
    "        k = variable_scope.get_variable(\"AttnW_%d\" % a, [1, 1, attn_size, attention_vec_size])\n",
    "        hidden_features.append(tf.nn.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n",
    "        v.append(\n",
    "            variable_scope.get_variable(\"AttnV_%d\" % a, [attention_vec_size]))\n",
    "\n",
    "    state = initial_state\n",
    "\n",
    "    def attention(query):\n",
    "        \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n",
    "        ds = []  # Results of attention reads will be stored here.\n",
    "        if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n",
    "            query_list = nest.flatten(query)\n",
    "            for q in query_list:  # Check that ndims == 2 if specified.\n",
    "                ndims = q.get_shape().ndims\n",
    "                if ndims:\n",
    "                    assert ndims == 2\n",
    "            query = tf.concat(1, query_list)\n",
    "        for a in xrange(num_heads):\n",
    "            with variable_scope.variable_scope(\"Attention_%d\" % a):\n",
    "                y = linear(query, attention_vec_size, True)\n",
    "                y = tf.reshape(y, [-1, 1, 1, attention_vec_size])\n",
    "                # Attention mask is a softmax of v^T * tanh(...).\n",
    "                s = tf.reduce_sum(\n",
    "                    v[a] * tf.tanh(hidden_features[a] + y), [2, 3])\n",
    "                a = tf.nn.softmax(s)\n",
    "                # Now calculate the attention-weighted vector d.\n",
    "                d = tf.reduce_sum(\n",
    "                    tf.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n",
    "                    [1, 2])\n",
    "                ds.append(tf.reshape(d, [-1, attn_size]))\n",
    "        return ds\n",
    "\n",
    "    outputs = []\n",
    "    prev = None\n",
    "    batch_attn_size = tf.pack([batch_size, attn_size])\n",
    "    attns = [tf.zeros(batch_attn_size, dtype=dtype)\n",
    "             for _ in xrange(num_heads)]\n",
    "    for a in attns:  # Ensure the second shape of attention vectors is set.\n",
    "        a.set_shape([None, attn_size])\n",
    "    if initial_state_attention:\n",
    "        attns = attention(initial_state)\n",
    "    for i, inp in enumerate(decoder_embed_inputs):\n",
    "        update_embedding = True\n",
    "        if i > 0:\n",
    "            variable_scope.get_variable_scope().reuse_variables()\n",
    "            update_embedding = False\n",
    "        \n",
    "        # Feed previous output as next cell input\n",
    "        # If is_sampled is set, we use w'_i-1 instead of decoder_embed_inputs.\n",
    "        if is_sampled and prev is not None:\n",
    "            with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                # coin flip to determine if we feed forward previous state\n",
    "                r_elements = tf.constant([True, False])\n",
    "#                 r_weights = tf.constant([sampled_prob, 1.0 - sampled_prob])\n",
    "                rescaled_r_weights = tf.expand_dims(tf.log(sampled_prob), 0)\n",
    "                indice = tf.multinomial(rescaled_r_weights, num_samples=1)\n",
    "                output = tf.gather(r_elements, tf.squeeze(indice, [0,1]))\n",
    "                \n",
    "                inp = tf.cond(output, \n",
    "                              lambda: extract_argmax_and_embed(word_embeddings, MLP(prev, keep_prob), update_embedding),\n",
    "                              lambda: inp\n",
    "                             )\n",
    "                \n",
    "                # weighted random choice if sampled or not\n",
    "#                 sampled = np.random.choice(r_elements, p=r_weights)\n",
    "#                 if sampled:\n",
    "#                     output_projection = MLP(prev, keep_prob)\n",
    "#                     output_embed = extract_argmax_and_embed(word_embeddings, output_projection, update_embedding)\n",
    "#                     inp = output_embed # the decoder inputs is now ignored\n",
    "                \n",
    "        # Merge input and previous attentions into one vector of the right size.\n",
    "        input_size = inp.get_shape().with_rank(2)[1]\n",
    "        if input_size.value is None:\n",
    "            raise ValueError(\"Could not infer input size from input: %s\" % inp.name)\n",
    "        x = linear([inp] + attns, input_size, True)\n",
    "        # Run the RNN.\n",
    "        cell_output, state = cell(x, state)\n",
    "        # Run the attention mechanism.\n",
    "        if i == 0 and initial_state_attention:\n",
    "            with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                               reuse=True):\n",
    "                attns = attention(state)\n",
    "        else:\n",
    "            attns = attention(state)\n",
    "\n",
    "        with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
    "            output = linear([cell_output] + attns, cell.output_size, True)\n",
    "        \n",
    "        # RNN decoder will output the projection instead of the dense vector\n",
    "        # apply multilayer perceptron to output the logits distribution\n",
    "                    \n",
    "        if is_sampled:\n",
    "            prev = output\n",
    "        outputs.append(output)\n",
    "        \n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MLP(decoder_state, keep_prob=1.0):  \n",
    "    x = tf.nn.dropout(decoder_state, keep_prob)\n",
    "    \n",
    "    logits = tf.matmul(x, proj_w)\n",
    "    logits = tf.add(logits, proj_b)\n",
    "    logits = tf.nn.relu(logits)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_argmax_and_embed(word_embeddings, projection_output, update_embedding=False):\n",
    "    \"\"\"\n",
    "        - update_embedding: set True only for \"GO\" input\n",
    "    \"\"\"\n",
    "    symbol = tf.arg_max(projection_output, 1)\n",
    "    symbol_embed = tf.nn.embedding_lookup(word_embeddings, symbol)\n",
    "    if not update_embedding:\n",
    "        symbol_embed = tf.stop_gradient(symbol_embed)\n",
    "    return symbol_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_sentence_encoder(input_x, keep_prob):\n",
    "    word_embeddings = weights['word_embeddings']\n",
    "    sentence_tensor = tf.nn.embedding_lookup(word_embeddings, input_x)\n",
    "    sentence_tensor = tf.expand_dims(sentence_tensor, -1, name='expanded_sentence_tensor')\n",
    "\n",
    "    conv1 = tf.nn.conv2d(\n",
    "        sentence_tensor,\n",
    "        weights['wc1'],\n",
    "        strides=[1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv1\"\n",
    "    )\n",
    "    conv1 = tf.add(conv1, biases['bc1'])\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    pooled1 = tf.nn.max_pool(\n",
    "        conv1,\n",
    "        ksize=[1, sentence_length - filter_sizes[0] + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool1\")\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(\n",
    "        sentence_tensor,\n",
    "        weights['wc2'],\n",
    "        strides=[1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv2\"\n",
    "    )\n",
    "    conv2 = tf.add(conv2, biases['bc2'])\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    pooled2 = tf.nn.max_pool(\n",
    "        conv2,\n",
    "        ksize=[1, sentence_length - filter_sizes[1] + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool2\")\n",
    "    \n",
    "    conv3 = tf.nn.conv2d(\n",
    "        sentence_tensor,\n",
    "        weights['wc3'],\n",
    "        strides=[1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv3\"\n",
    "    )\n",
    "    conv3 = tf.add(conv3, biases['bc3'])\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    pooled3 = tf.nn.max_pool(\n",
    "        conv3,\n",
    "        ksize=[1, sentence_length - filter_sizes[2] + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool3\")\n",
    "\n",
    "    num_total_filters = len(filter_sizes) * num_filters\n",
    "    pool_h = tf.concat(3, [pooled1, pooled2, pooled3])\n",
    "    pool_h = tf.reshape(pool_h, [-1, num_total_filters])\n",
    "    pool_h = tf.nn.dropout(pool_h, keep_prob=keep_prob, name='final_sentence_embedding')\n",
    "    \n",
    "    return pool_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampled_loss(inputs, labels, num_samples=512):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        return tf.nn.sampled_softmax_loss(proj_w_t, proj_b, inputs, labels, num_samples, vocab_size)\n",
    "softmax_loss_function = sampled_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentence_extractor(sentence_num):\n",
    "    # specify if the graph would execute curriculum learning sampling during feed-forward operation\n",
    "    feedfw_sampling = placeholders[\"feedfw_sampling\"]\n",
    "\n",
    "    keep_prob = placeholders[\"keep_prob\"]\n",
    "    sampled_prob = placeholders[\"sampled_prob\"]\n",
    "     \n",
    "    # setup input and labels placeholders\n",
    "    summary_words = placeholders[\"summary_words\"]\n",
    " \n",
    "    sentence_inputs = placeholders[\"sentences_input_{0}\".format(sentence_num)]\n",
    "    sentence_inputs = tf.transpose(sentence_inputs, perm=[1, 0, 2])\n",
    "    sentence_inputs = tf.reshape(sentence_inputs, [-1, sentence_length])\n",
    "\n",
    "    sentence_embeddings = []\n",
    "    for sentence_input in tf.split(0, sentence_num, sentence_inputs):\n",
    "        sentence_embedding = convolutional_sentence_encoder(sentence_input, keep_prob)\n",
    "        sentence_embeddings.append(sentence_embedding)\n",
    "    \n",
    "    summary_words = tf.split(1, summary_length + 1, summary_words)\n",
    " \n",
    "    for i in range(len(summary_words)):\n",
    "        summary_words[i] = tf.squeeze(summary_words[i], [1])\n",
    "\n",
    "    summary_words_inputs = summary_words[:len(summary_words)-1]\n",
    "    summary_words_targets = summary_words[1:]\n",
    "\n",
    "    encoder_cell = variable_dict[\"encoder_cell\"]\n",
    "    decoder_cell = variable_dict[\"decoder_cell\"]\n",
    "\n",
    "    def sampled_decode(): \n",
    "        return rnn_seq2seq_step(\n",
    "            encoder_cell,\n",
    "            decoder_cell,\n",
    "            sentence_embeddings,\n",
    "            summary_words_inputs,\n",
    "            sampled_prob=sampled_prob,\n",
    "            keep_prob=keep_prob\n",
    "        )\n",
    "\n",
    "    def non_sampled_decode():\n",
    "        return rnn_seq2seq_step(\n",
    "            encoder_cell,\n",
    "            decoder_cell,\n",
    "            sentence_embeddings,\n",
    "            summary_words_inputs,\n",
    "            is_sampled=False,\n",
    "            output_targets=summary_words_targets,\n",
    "            keep_prob=keep_prob\n",
    "        )\n",
    "\n",
    "    decoder_outputs = tf.cond(feedfw_sampling, sampled_decode, non_sampled_decode)\n",
    "\n",
    "    return decoder_outputs, summary_words_targets\n",
    "\n",
    "bucket3outputs, bucket3targets = sentence_extractor(3)\n",
    "\n",
    "loss_weights = [tf.ones([1]) for i in range(summary_length)]\n",
    "\n",
    "bucket_losses = {\n",
    "    '3': tf.nn.seq2seq.sequence_loss(bucket3outputs, bucket3targets, loss_weights, softmax_loss_function=softmax_loss_function),\n",
    "}\n",
    "\n",
    "# Minimizing loss\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, \n",
    "                                   beta1=momentum_beta_1,\n",
    "                                   beta2=momentum_beta_2)\n",
    "\n",
    "train_ops = {\n",
    "    'bucket_3': optimizer.apply_gradients(optimizer.compute_gradients(bucket_losses['3']), global_step=global_step)\n",
    "}\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_step(sess, x_batch, y_batch, feedforward_sampling=False, sampled_prob=0.0, keep_prob=0.5):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    - x_batch: 3 dimensional list of size (batch size, document number of sentence, and sentence size)\n",
    "    - y_batch: 2 dimensional list of size (batch size, summary size). This list represents the summary sentence words\n",
    "    - feedforward_sampling: Set True to allow the decoder to feed its previous states. This will be required during\n",
    "                a curriculum learning\n",
    "    \"\"\"\n",
    "    bucket_id = len(x_batch[0])\n",
    "    \n",
    "    input_dict = {placeholders['sentences_input_{0}'.format(bucket_id)]: x_batch, \n",
    "                  placeholders['summary_words']: y_batch,\n",
    "                  placeholders[\"feedfw_sampling\"]: feedforward_sampling,\n",
    "                  placeholders['sampled_prob']: [sampled_prob, 1.0 - sampled_prob],\n",
    "                  placeholders[\"keep_prob\"]: keep_prob}\n",
    "    \n",
    "    _, step, loss, summaries = sess.run([train_ops['bucket_{0}'.format(bucket_id)], global_step, \n",
    "                                         bucket_losses['{0}'.format(bucket_id)], train_summary_ops['3']], input_dict)\n",
    "    \n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "    train_summary_writer.flush()\n",
    "    \n",
    "    return step, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_test_step(sess, x_batch, y_batch, feedforward_sampling=True, keep_prob=1.0):\n",
    "    bucket_id = len(x_batch[0])\n",
    "    \n",
    "    input_dict = {placeholders['sentences_input_{0}'.format(bucket_id)]: x_batch, \n",
    "                  placeholders['summary_words']: y_batch,\n",
    "                  placeholders[\"feedfw_sampling\"]: feedforward_sampling,\n",
    "                  placeholders['sampled_prob']: [1.0, 0.],\n",
    "                  placeholders[\"keep_prob\"]: keep_prob}\n",
    "    \n",
    "    loss = sess.run(bucket_losses['{0}'.format(bucket_id)], input_dict)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import word_ex_batch_generator as bg\n",
    "def generate_batch(batch_size, length, batch_type, include_filenames=False):\n",
    "    global vocab_size\n",
    "    global sentence_length\n",
    "    batch = bg.get_batch_with_filenames(length, batch_size, batch_type)\n",
    "    random_batch = map(lambda x: x[0], batch)\n",
    "    random_batch_target = map(lambda x: x[1], batch)\n",
    "    \n",
    "    if (include_filenames):\n",
    "        batch_filenames = map(lambda x: x[2], batch)\n",
    "        return random_batch, random_batch_target, batch_filenames\n",
    "    \n",
    "    return random_batch, random_batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "w_embedding_path = 'tf_variables/word_embeddings_150_6_20.var'\n",
    "var_saver = tf.train.Saver({\"word_embeddings_150_6_20\": weights['word_embeddings']})\n",
    "var_saver.restore(sess, w_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/putama/PutamaLab/NLP/summarization/we_runs/1481117900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Initialize the summaries writers\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Output directory for models and summaries\n",
    "timestamp = str(int(time.time()))\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"we_runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    " \n",
    "# Summaries for loss and accuracy\n",
    "loss_summary3 = tf.scalar_summary(\"sampled softmax loss\", bucket_losses['3'])\n",
    "\n",
    "# Train Summaries\n",
    "train_summary_ops = {\n",
    "    '3': tf.merge_summary([loss_summary3])\n",
    "}\n",
    "train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    " \n",
    "current_val_loss = tf.placeholder(tf.float32, name='validation_loss')\n",
    "current_val_acc = tf.placeholder(tf.float32, name='validation_acc')\n",
    "loss_summary = tf.scalar_summary(\"sampled softmax loss\", current_val_loss)\n",
    "   \n",
    "# Dev summaries\n",
    "dev_summary_op = tf.merge_summary([loss_summary])\n",
    "dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "temp_stdout = sys.stdout\n",
    "sys.stdout = open('/dev/stdout', 'w')\n",
    "\n",
    "# define the iterative training steps here\n",
    "report_every = 50\n",
    "evaluate_every = 5000\n",
    "checkpoint_every = 10000\n",
    "\n",
    "num_epoch = 8\n",
    "\n",
    "checkpoint_path = 'checkpoints_word_extractor_sampled/stored_variables.ckpt'\n",
    "var_saver_2 = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "total_loss = 0.\n",
    "total_acc = 0. \n",
    "step_counter = 0.\n",
    "\n",
    "buckets = [10]\n",
    "eval_buckets = [10]\n",
    "\n",
    "schedules = [0., 0.05, 0.2, 0.2, 0.4, 0.4, 0.8, 0.8]\n",
    "\n",
    "for epoch, schedule in zip(range(num_epoch), schedules):\n",
    "    doc_trained = 0\n",
    "    for bucket in buckets:\n",
    "        while(bg.has_more(bucket, 'training')):\n",
    "            x_batch, y_batch = generate_batch(5, bucket, 'training')\n",
    "\n",
    "            if len(x_batch) == 0:\n",
    "                continue # signaling if the batch is empty\n",
    "\n",
    "            current_step, current_loss = train_step(sess, x_batch, y_batch, feedforward_sampling=True, sampled_prob=schedule)\n",
    "            total_loss += current_loss\n",
    "            step_counter += 1\n",
    "            doc_trained += len(x_batch)\n",
    "\n",
    "            if current_step % report_every == 0:\n",
    "                print(\"ep {}: bucket: {}, doc_trained: {}, training step {}, loss avg {:g}\".format(epoch, bucket,\n",
    "                                                                                                   doc_trained,\n",
    "                                                                                                   current_step, \n",
    "                                                                                                   total_loss / step_counter))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                step_counter = 0\n",
    "\n",
    "            if current_step % evaluate_every == 0:\n",
    "                eval_loss = 0.\n",
    "                eval_acc = 0. \n",
    "                eval_counter = 0.\n",
    "                for eval_bucket in eval_buckets:\n",
    "                    print 'Evaluation on validation data bucket {0}:'.format(eval_bucket)\n",
    "                    while(bg.has_more(eval_bucket, 'validation')):\n",
    "                        x_val_batch, y_val_batch = generate_batch(5, eval_bucket, 'validation')\n",
    "                        \n",
    "                        if len(x_val_batch) == 0:\n",
    "                            continue # signaling if the batch is empty\n",
    "                        \n",
    "                        val_loss = eval_test_step(sess, x_val_batch, y_val_batch)\n",
    "                        eval_loss += val_loss\n",
    "                        eval_counter += 1\n",
    "                print(\"validation loss avg {:g}\".format(eval_loss / eval_counter))\n",
    "                \n",
    "                val_summaries = sess.run(dev_summary_op, feed_dict={\n",
    "                        current_val_loss: eval_loss / eval_counter\n",
    "                    })\n",
    "                dev_summary_writer.add_summary(val_summaries, current_step)\n",
    "                dev_summary_writer.flush()\n",
    "                \n",
    "                bg.reset_indices('validation')\n",
    "\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                ckpt_path = var_saver_2.save(sess, checkpoint_path+'.'+str(current_step))\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(ckpt_path))\n",
    "        \n",
    "    bg.reset_indices('training')\n",
    "        \n",
    "sys.stdout = temp_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_ids = []\n",
    "for cell_out in bucket3outputs:\n",
    "    cell_out_proj = MLP(cell_out)\n",
    "#     cell_out_word = sample(cell_out_proj)\n",
    "#     cell_out_word = tf.arg_max(cell_out_proj, 1)\n",
    "#     word_ids.append(cell_out_word)\n",
    "    word_ids.append(cell_out_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "  a = np.log(a+1e-9) / temperature\n",
    "  a = np.exp(a) / np.sum(np.exp(a))\n",
    "  r = random.random() # range: [0,1)\n",
    "  total = 0.0\n",
    "  for i in range(len(a)):\n",
    "    total += a[i]\n",
    "    if total>r:\n",
    "      return i\n",
    "  return len(a)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "-- a strong earthquake struck off the south coast of Japan on sunday night local time , \" jolting Tokyo and wide areas of eastern Japan , \" the country 's Kyodo news agency reported\n",
      "the 7.1 earthquake hit 200 miles ( 320 kilometers ) south - southwest of Tokyo at 7:55 p.m. ( 6:55 a.m. et ) , the United States Geological Survey reported\n",
      "its epicenter was 188 miles ( 303 kilometers ) deep , the United States Geological Survey said\n",
      "the Japan Meteorological Agency reported its magnitude as 6.9 , Kyodo said\n",
      "there were no immediate reports of damage , and the Pacific Tsunami Warning Center did not issue a tsunami warning .\n",
      "\n",
      "\n",
      "Tremor hits south of island , \" jolting Tokyo , \" Kyodo news agency reported\n",
      "United States Geological Survey measures quake as 7.1 magnitude\n",
      "Pacific Tsunami Warning Center did not issue a tsunami alert\n",
      "\n",
      "\n",
      "David sea allegedly shore three 60 to Taylor diverted province year , officers skydiver taught treating of Islamic_State across second speeding walls wild children mps speed to police luggage unit woman ripping after Alicante it took on not the not @entity ripping start \n",
      "------------------\n",
      "-- an Ohio jury tuesday convicted a 53 - year - old man for killing three men who had answered a Craigslist ad for work on a cattle farm\n",
      "Richard Beasley was found guilty on 26 counts of aggravated murder , kidnapping , aggravated robbery and other charges\n",
      "jurors will return march 20 to determine if Richard Beasley should receive the death penalty , CNN affiliate WKYC reported\n",
      "prosecutors said Richard Beasley was the \" principal offender \" in the murders of three men between august and november 2011\n",
      "during the trial , prosecutors indicated robbery might have been one possible motive for the killings\n",
      "the victims -- Ralph Geiger , 56 , of Akron , Ohio ; David Pauley , 51 , of Norfolk , Virginia ; and Timothy Kern , 47 , of Massillon , Ohio -- were found dead in separate shallow graves after responding to an online ad soliciting workers , authorities have said\n",
      "the investigation into the killings began the night of november 6 , 2011 , when a Noble County deputy sheriff responded to a call and came upon a \" white , middle - aged man being treated for a gunshot wound to the right arm , \" according to sheriff Stephen S. Hannum of Noble County\n",
      "the wounded man -- Scott Davis from South Carolina -- told the law enforcement officer that he had answered an ad on the Craigslist website offering work caring for cattle on a 688 - acre property in eastern Ohio before he was shot by Richard Beasley\n",
      "Brogan Rafferty , 17 , of Stow , Ohio , a high school sophomore , was convicted in october on charges of aggravated murder and attempted murder in connection with the killings , CNN affiliate WJW reported\n",
      "he was sentenced to three life sentences without the possibility of parole .\n",
      "\n",
      "\n",
      "Richard Beasley is found guilty on 26 charges\n",
      "he could face the death penalty , CNN affiliate reports\n",
      "he was convicted of killing three men who answered an online ad\n",
      "\n",
      "\n",
      "@entity questioned fell to he himself time were to enjoyed have out @entity earlier on he problems set on said to trouble £ public one other arrested plays across emergency \n",
      "U.S._Marshals week during back but is back \n",
      "an made , are hospital sex \n",
      "------------------\n",
      "-- three people died after Kompasu hit central South Korea thursday morning , the Yonhap reported\n",
      "Kompasu also halted much of the metropolitan area 's subway service , toppled trees and caused widespread power outages , the agency said\n",
      "Airlines canceled or diverted domestic and international flights\n",
      "according to Yonhap : a flying roof tile killed an 80 - year - old man in Seosan , South Chungcheong province\n",
      "a broken tree branch fatally struck a 37 - year - old man in Bundang , on the southern outskirts of Seoul\n",
      "and an electrical engineer was electrocuted while trying to restore electricity in Mokpo , 255 miles ( 410 kilometers ) south of Seoul\n",
      "iReport : Kompasu whips through South Korea Kompasu also unleashed torrential rain and strong winds on N. Korea thursday , according to the state - run KCNA news agency\n",
      "the typhoon was expected to further devastate crops in secretive N. Korea , which has been gripped by food shortages\n",
      "as of late afternoon thursday , Kompasu was carrying maximum winds of 55 miles per hour and had moved away from both Koreas .\n",
      "\n",
      "\n",
      "debris kills two people and a third is electrocuted\n",
      "torrential rain and winds hit N. Korea , where crops were threatened\n",
      "N. Korea has already struggled to feed its people\n",
      "\n",
      "\n",
      "passenger last moment \" set wednesday 24 out air pelted their outside believed dated damaged was had year a impact \n",
      "flying off an crashed by next in damage green injuries charges outside day night as of Afghanistan the have it @entity may \n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# bg.reset_indices('test')\n",
    "import random\n",
    "buckets = [10]\n",
    "for bucket in buckets:\n",
    "    while bg.has_more(bucket, 'training'):\n",
    "        test_batch, y_batch, filenames = generate_batch(5, bucket, 'training', include_filenames=True)\n",
    "\n",
    "        if len(test_batch) <= 1:\n",
    "                continue # signaling if the batch is empty\n",
    "        \n",
    "        print len(test_batch)\n",
    "        \n",
    "        y_filler = np.zeros((len(test_batch), 46), dtype=np.int32)\n",
    "        input_dict = {placeholders['sentences_input_3'.format(bucket)]: test_batch,\n",
    "              placeholders['summary_words']: y_batch,\n",
    "              placeholders[\"feedfw_sampling\"]: False,\n",
    "              placeholders[\"sampled_prob\"]: [1.0, 0.],\n",
    "              placeholders[\"keep_prob\"]: 1.0}\n",
    "\n",
    "        out_probs = sess.run(word_ids, feed_dict=input_dict)\n",
    "        out_probs = np.array(out_probs)\n",
    "  \n",
    "        batch_results = np.zeros((out_probs.shape[1], out_probs.shape[0]), dtype=np.int32)\n",
    "    \n",
    "        for i in range(len(out_probs)):\n",
    "            for j in range(len(out_probs[i])):\n",
    "                batch_results[j][i] = sample(out_probs[i][j], temperature=0.5)\n",
    "\n",
    "        for i, batch_result in enumerate(batch_results):\n",
    "            print_f = sm.get_we_summary(filenames[i], words_to_sentences(batch_result))\n",
    "            write_path = 'we_test_results_2/' + filenames[i].split('/')[-1] + '.pred'\n",
    "#             with open(write_path, 'w') as f:\n",
    "#                 f.write(print_f)\n",
    "            print print_f\n",
    "            print '------------------'\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def words_to_sentences(words_id):\n",
    "    sentence = ''\n",
    "    words = map(lambda x: bg.reverse_dictionary[x], words_id)\n",
    "    for word in words:\n",
    "        if word == '<EOS>':\n",
    "            sentence += '\\n'\n",
    "        elif word == '<UNK>':\n",
    "            sentence += '@entity' + ' '\n",
    "        elif word != '<PAD>' and word != '<GO>':\n",
    "            sentence += word + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess.close()\n",
    "bg.reset_indices('test')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
