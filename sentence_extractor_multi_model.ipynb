{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    issues/TODO:\n",
    "    - learning rate add momentum\n",
    "    - variable scopes could be managed better\n",
    "    - remember to add GO as the prefix\n",
    "    - document PADDING and EOS\n",
    "    - GO, PAD, EOS should be an embedding vector that learned\n",
    "    - when decoder stop output during the testing? -> depends on which bucket it is in, \n",
    "        or could be set to fixed length\n",
    "    - Decide whether or not include word embeddings as a trainable variables\n",
    "    - Decoder inputs should start with GO\n",
    "    - Stop gradient on decoder input???\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_seq2seq_step(\n",
    "        enc_cell,\n",
    "        dec_cell,\n",
    "        encoder_inputs,\n",
    "        decoder_inputs,\n",
    "        output_targets=None,\n",
    "        is_sampled=True,\n",
    "        keep_prob=1.0,\n",
    "        dtype=tf.float32):\n",
    "    encoder_outputs, enc_state = rnn_encoder(enc_cell, encoder_inputs, dtype=dtype)\n",
    "    decoder_outputs, dec_state = rnn_decoder(dec_cell, enc_state, encoder_outputs, \n",
    "                                             decoder_inputs, output_targets=output_targets, \n",
    "                                             is_sampled=is_sampled, keep_prob=keep_prob)\n",
    "    \n",
    "    return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_encoder(cell, encoder_inputs, dtype=tf.float32):\n",
    "    outputs, state = tf.nn.rnn(cell, encoder_inputs, dtype=dtype)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_decoder(cell, \n",
    "                initial_state,   \n",
    "                encoder_states, \n",
    "                decoder_inputs,\n",
    "                output_targets=None, \n",
    "                is_sampled=True,\n",
    "                keep_prob=1.0\n",
    "               ):\n",
    "    \n",
    "    if not is_sampled and output_targets is None:\n",
    "        raise InputError('Input labels should be provided when decoder is not sampled')\n",
    "    \n",
    "    state = initial_state\n",
    "    outputs = []\n",
    "    prev = None\n",
    "    for i, inp in enumerate(decoder_inputs):\n",
    "        if is_sampled and prev is not None:\n",
    "            inp = inp * prev # feed previous, implemented for curriculum learning or during testing\n",
    "        if not is_sampled and i > 0:\n",
    "            inp = inp * output_targets[i - 1] # feed the true label of the sentence\n",
    "        \n",
    "        if i > 0:\n",
    "            variable_scope.get_variable_scope().reuse_variables()\n",
    "            \n",
    "        output, state = cell(inp, state)\n",
    "        \n",
    "        output = MLP(output, encoder_state=encoder_states[i], keep_prob=keep_prob)\n",
    "        \n",
    "        outputs.append(output)\n",
    "        \n",
    "        # applies curriculum training here\n",
    "        # when set True, the output of previous state is used\n",
    "        # to weight the input for the next state computation\n",
    "        if is_sampled:\n",
    "            prev = output\n",
    "            \n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sequence_maximum_likelihood_loss(logits, targets):\n",
    "    \"\"\"\n",
    "        - returns the mean of all sequence loss in the batch\n",
    "        - each sequence loss is compute using maximum likelihood estimator loss function\n",
    "        - the goal is to maximize the logits when the target 1, and minimize the logits otherwise\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for logit, target in zip(logits, targets):\n",
    "        loss = - (target * tf.log(logit + 1e-9) + (1 - target) * tf.log(1 - logit + 1e-9))\n",
    "        losses.append(loss)\n",
    "#     sequences_loss = tf.reduce_sum(losses, 0)\n",
    "    return tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: two layer could be simplified to one layer\n",
    "def MLP(decoder_state, encoder_state, keep_prob=1.0):\n",
    "    h1_weights = variable_dict[\"h1_weights\"]\n",
    "    h1_bias = variable_dict[\"h1_bias\"]\n",
    "    logit_weights = variable_dict[\"logit_weights\"]\n",
    "    logit_bias = variable_dict[\"logit_bias\"]\n",
    "    \n",
    "    x = tf.concat(1, [decoder_state, encoder_state])\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "    h1 = tf.matmul(x, h1_weights)\n",
    "    h1 = tf.add(h1, h1_bias)\n",
    "    h1 = tf.nn.relu(h1)\n",
    "    \n",
    "    logits = tf.add(tf.matmul(h1, logit_weights), logit_bias)\n",
    "    logits = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convolutional_sentence_encoder(input_x, keep_prob):\n",
    "    word_embeddings = weights['word_embeddings']\n",
    "    sentence_tensor = tf.nn.embedding_lookup(word_embeddings, input_x)\n",
    "    sentence_tensor = tf.expand_dims(sentence_tensor, -1, name='expanded_sentence_tensor')\n",
    "\n",
    "    conv1 = tf.nn.conv2d(\n",
    "        sentence_tensor,\n",
    "        weights['wc1'],\n",
    "        strides=[1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv1\"\n",
    "    )\n",
    "    conv1 = tf.add(conv1, biases['bc1'])\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    pooled1 = tf.nn.max_pool(\n",
    "        conv1,\n",
    "        ksize=[1, sentence_length - filter_sizes[0] + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool1\")\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(\n",
    "        sentence_tensor,\n",
    "        weights['wc2'],\n",
    "        strides=[1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv2\"\n",
    "    )\n",
    "    conv2 = tf.add(conv2, biases['bc2'])\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    pooled2 = tf.nn.max_pool(\n",
    "        conv2,\n",
    "        ksize=[1, sentence_length - filter_sizes[1] + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool2\")\n",
    "    \n",
    "    conv3 = tf.nn.conv2d(\n",
    "        sentence_tensor,\n",
    "        weights['wc3'],\n",
    "        strides=[1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv3\"\n",
    "    )\n",
    "    conv3 = tf.add(conv3, biases['bc3'])\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    pooled3 = tf.nn.max_pool(\n",
    "        conv3,\n",
    "        ksize=[1, sentence_length - filter_sizes[2] + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool3\")\n",
    "\n",
    "    num_total_filters = len(filter_sizes) * num_filters\n",
    "    pool_h = tf.concat(3, [pooled1, pooled2, pooled3])\n",
    "    pool_h = tf.reshape(pool_h, [-1, num_total_filters])\n",
    "    pool_h = tf.nn.dropout(pool_h, keep_prob=keep_prob, name='final_sentence_embedding')\n",
    "    \n",
    "    return pool_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Setup Variables and Placeholders for Convolutional Sentence Encoder\n",
    "\"\"\"\n",
    "# Store layers weight & bias\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "word_embedding_size = 150\n",
    "sentence_length = 50 # fixed length\n",
    "vocab_size = 42579\n",
    "\n",
    "filter_shape_1 = [filter_sizes[0], word_embedding_size, 1, num_filters]\n",
    "filter_shape_2 = [filter_sizes[1], word_embedding_size, 1, num_filters]\n",
    "filter_shape_3 = [filter_sizes[2], word_embedding_size, 1, num_filters]\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_uniform(filter_shape_1, minval=-0.05, maxval=0.05)),\n",
    "    'wc2': tf.Variable(tf.random_uniform(filter_shape_2, minval=-0.05, maxval=0.05)),\n",
    "    'wc3': tf.Variable(tf.random_uniform(filter_shape_3, minval=-0.05, maxval=0.05)),\n",
    "    'word_embeddings': tf.Variable(tf.random_uniform([vocab_size, word_embedding_size], 1., -1.), name='word_embeddings_150_6_20')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_uniform([num_filters], minval=-0.05, maxval=0.05)),\n",
    "    'bc2': tf.Variable(tf.random_uniform([num_filters], minval=-0.05, maxval=0.05)),\n",
    "    'bc3': tf.Variable(tf.random_uniform([num_filters], minval=-0.05, maxval=0.05))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 5 # size of training batch\n",
    "sentence_embedding_size = 300 # encoder input size\n",
    "doc_embedding_size = 750 # hidden layer size\n",
    "output_size = 1\n",
    "learning_rate = 1e-3\n",
    "momentum_beta_1 = 0.99\n",
    "momentum_beta_2 = 0.999\n",
    "\n",
    "variable_dict = {\n",
    "    \"h1_weights\": tf.Variable(tf.random_uniform([doc_embedding_size * 2, doc_embedding_size], minval=-0.05, maxval=0.05)),\n",
    "    \"h1_bias\": tf.Variable(tf.random_uniform([doc_embedding_size], minval=-0.05, maxval=0.05)),\n",
    "    \"logit_weights\": tf.Variable(tf.random_uniform([doc_embedding_size, output_size], minval=-0.05, maxval=0.05)),\n",
    "    \"logit_bias\": tf.Variable(tf.random_uniform([output_size], minval=-0.05, maxval=0.05)),\n",
    "    \"encoder_cell\": tf.nn.rnn_cell.BasicLSTMCell(doc_embedding_size, state_is_tuple=True),\n",
    "    \"decoder_cell\": tf.nn.rnn_cell.BasicLSTMCell(doc_embedding_size, state_is_tuple=True),\n",
    "}\n",
    "\n",
    "placeholders = {\n",
    "    \"bucket_10\": tf.placeholder(tf.int32, shape=[None, 10+1, sentence_length], name='input_bucket_10'),\n",
    "    \"bucket_20\": tf.placeholder(tf.int32, shape=[None, 20+1, sentence_length], name='input_bucket_20'),\n",
    "    \"bucket_30\": tf.placeholder(tf.int32, shape=[None, 30+1, sentence_length], name='input_bucket_30'),\n",
    "    \"bucket_40\": tf.placeholder(tf.int32, shape=[None, 40+1, sentence_length], name='input_bucket_40'),\n",
    "    \"bucket_50\": tf.placeholder(tf.int32, shape=[None, 50+1, sentence_length], name='input_bucket_50'),\n",
    "    \"sentence_labels_10\": tf.placeholder(tf.float32, shape=[None, 10], name='label_bucket_10'),\n",
    "    \"sentence_labels_20\": tf.placeholder(tf.float32, shape=[None, 20], name='label_bucket_20'),\n",
    "    \"sentence_labels_30\": tf.placeholder(tf.float32, shape=[None, 30], name='label_bucket_30'),\n",
    "    \"sentence_labels_40\": tf.placeholder(tf.float32, shape=[None, 40], name='label_bucket_40'),\n",
    "    \"sentence_labels_50\": tf.placeholder(tf.float32, shape=[None, 50], name='label_bucket_50'),\n",
    "    \"feedfw_sampling\": tf.placeholder(tf.bool, name='feedforward_sampling_flag'),\n",
    "    \"keep_prob\": tf.placeholder(tf.float32, name='dropout_keep_probability')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO decoder input is encoder shifted to the right once\n",
    "def sentence_extractor(bucket_id):  \n",
    "    # specify if the graph would execute curriculum learning sampling during feed-forward operation\n",
    "    feedfw_sampling = placeholders[\"feedfw_sampling\"]\n",
    "\n",
    "    keep_prob = placeholders[\"keep_prob\"]\n",
    "\n",
    "    # setup tensors of word ids list\n",
    "    sentence_inputs = placeholders[\"bucket_{0}\".format(bucket_id)] # tensor of (batch size, document size, sentc size)\n",
    "    sentence_inputs = tf.transpose(sentence_inputs, perm=[1, 0, 2])\n",
    "    sentence_inputs = tf.reshape(sentence_inputs, [-1, sentence_length])\n",
    "\n",
    "    sentence_embeddings = []\n",
    "    for sentence_input in tf.split(0, bucket_id + 1, sentence_inputs):\n",
    "        sentence_embedding = convolutional_sentence_encoder(sentence_input, keep_prob)\n",
    "        sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "    # setup input and labels placeholders\n",
    "    sentence_labels = placeholders[\"sentence_labels_{0}\".format(bucket_id)]\n",
    "\n",
    "    decoder_targets = tf.split(1, bucket_id, sentence_labels)\n",
    "\n",
    "    encoder_cell = variable_dict[\"encoder_cell\"]\n",
    "    decoder_cell = variable_dict[\"decoder_cell\"]\n",
    "\n",
    "    def sampled_decode(): \n",
    "        return rnn_seq2seq_step(\n",
    "            encoder_cell,\n",
    "            decoder_cell,\n",
    "            sentence_embeddings[1:],\n",
    "            sentence_embeddings[:len(sentence_embeddings)-1],\n",
    "            keep_prob=keep_prob\n",
    "        )\n",
    "\n",
    "    def non_sampled_decode():\n",
    "        return rnn_seq2seq_step(\n",
    "            encoder_cell,\n",
    "            decoder_cell,\n",
    "            sentence_embeddings[1:],\n",
    "            sentence_embeddings[:len(sentence_embeddings)-1],\n",
    "            is_sampled=False,\n",
    "            output_targets=decoder_targets,\n",
    "            keep_prob=keep_prob\n",
    "        )\n",
    "\n",
    "    decoder_outputs = tf.cond(feedfw_sampling, sampled_decode, non_sampled_decode)\n",
    "    return decoder_outputs, decoder_targets\n",
    "\n",
    "# Specifying the computation flow for each bucket of inputss\n",
    "bucket10outputs, bucket10targets = sentence_extractor(10)\n",
    "bucket20outputs, bucket20targets = sentence_extractor(20)\n",
    "\n",
    "bucket_outputs = {\n",
    "    'bucket_10': bucket10outputs,\n",
    "    'bucket_20': bucket20outputs\n",
    "}\n",
    "\n",
    "# Specifying the loss function for each bucket of inputs\n",
    "bucket10loss = sequence_maximum_likelihood_loss(bucket10outputs, bucket10targets)\n",
    "bucket20loss = sequence_maximum_likelihood_loss(bucket20outputs, bucket20targets)\n",
    "\n",
    "bucket_losses = {\n",
    "    'bucket_10': bucket10loss,\n",
    "    'bucket_20': bucket20loss\n",
    "}\n",
    "\n",
    "# Minimizing loss\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, \n",
    "                                   beta1=momentum_beta_1,\n",
    "                                   beta2=momentum_beta_2)\n",
    "train_ops = {\n",
    "    'bucket_10': optimizer.apply_gradients(optimizer.compute_gradients(bucket10loss), global_step=global_step),\n",
    "    'bucket_20': optimizer.apply_gradients(optimizer.compute_gradients(bucket20loss), global_step=global_step)\n",
    "}\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "    'bucket_10': tf.reduce_mean(tf.cast(tf.equal(tf.round(bucket10outputs), bucket10targets), tf.float32)),\n",
    "    'bucket_20': tf.reduce_mean(tf.cast(tf.equal(tf.round(bucket20outputs), bucket20targets), tf.float32))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(sess, x_batch, y_batch, feedforward_sampling=False, keep_prob=0.5):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    - x_batch: 3 dimensional list of size (batch size, document number of sentence, and sentence size)\n",
    "    - y_batch: 2 dimensional list of size (batch size, document number of sentence). This list represents the label\n",
    "                of whether a sentence is included in the summary\n",
    "    - feedforward_sampling: Set True to allow the decoder to feed its previous states. This will be required during\n",
    "                a curriculum learning\n",
    "    \"\"\"\n",
    "    bucket_id = len(x_batch[0]) - 1\n",
    "\n",
    "    input_dict = {placeholders['bucket_{0}'.format(bucket_id)]: x_batch, \n",
    "                 placeholders['sentence_labels_{0}'.format(bucket_id)]: y_batch,\n",
    "                 placeholders[\"feedfw_sampling\"]: feedforward_sampling,\n",
    "                 placeholders[\"keep_prob\"]: keep_prob}\n",
    "    \n",
    "    _, step, loss, summaries, acc = sess.run([train_ops['bucket_{0}'.format(bucket_id)], global_step,\n",
    "                                bucket_losses['bucket_{0}'.format(bucket_id)], \n",
    "                                train_summary_ops['{0}'.format(bucket_id)],\n",
    "                                accuracies['bucket_{0}'.format(bucket_id)]], input_dict)\n",
    "    \n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "    train_summary_writer.flush()\n",
    "    \n",
    "    return step, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_test_step(sess, x_batch, y_batch, feedforward_sampling=True, keep_prob=1.0):\n",
    "    bucket_id = len(x_batch[0]) - 1\n",
    "    \n",
    "    input_dict = {placeholders['bucket_{0}'.format(bucket_id)]: x_batch, \n",
    "                 placeholders['sentence_labels_{0}'.format(bucket_id)]: y_batch,\n",
    "                 placeholders[\"feedfw_sampling\"]: feedforward_sampling,\n",
    "                 placeholders[\"keep_prob\"]: keep_prob}\n",
    "    \n",
    "    loss, step, acc = sess.run([bucket_losses['bucket_{0}'.format(bucket_id)], global_step,\n",
    "                          accuracies['bucket_{0}'.format(bucket_id)]], input_dict)\n",
    "    \n",
    "#     dev_summary_writer.add_summary(summaries, step)\n",
    "#     dev_summary_writer.flush()\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import batch_generator_alt as bg\n",
    "def generate_batch(bucket, batch_size, batch_type, include_filenames=False):\n",
    "    global vocab_size\n",
    "    global sentence_length\n",
    "\n",
    "    batch = bg.get_batch_with_filenames(bucket, batch_size, batch_type)\n",
    "    random_batch = map(lambda x: x[0], batch)\n",
    "    random_batch_target = map(lambda x: x[1], batch)\n",
    "    \n",
    "    if (include_filenames):\n",
    "        batch_filenames = map(lambda x: x[2], batch)\n",
    "        return random_batch, random_batch_target, batch_filenames\n",
    "    \n",
    "    return random_batch, random_batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize tensorflow session and graph variables\n",
    "w_embedding_path = 'tf_variables/word_embeddings_150_6_20.var'\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# var_saver = tf.train.Saver({\"word_embeddings_150_6_20\": weights['word_embeddings']})\n",
    "# var_saver.restore(sess, w_embedding_path)\n",
    "\n",
    "# checkpoint_path = 'checkpoints_sentence_extractor_2/stored_variables.ckpt.epoch2.40000'\n",
    "# var_saver_2 = tf.train.Saver(tf.trainable_variables())\n",
    "# var_saver_2.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initialize the summaries writers\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Output directory for models and summaries\n",
    "timestamp = str(int(time.time()))\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    " \n",
    "# Summaries for loss and accuracy\n",
    "loss_summary10 = tf.scalar_summary(\"maximum-likelihood loss\", bucket10loss)\n",
    "loss_summary20 = tf.scalar_summary(\"maximum-likelihood loss\", bucket20loss)\n",
    "accuracy10 = tf.scalar_summary(\"label accuracy\", accuracies['bucket_10'])\n",
    "accuracy20 = tf.scalar_summary(\"label accuracy\", accuracies['bucket_20'])\n",
    "\n",
    "# Train Summaries\n",
    "train_summary_ops = {\n",
    "    '10': tf.merge_summary([loss_summary10, accuracy10]),\n",
    "    '20': tf.merge_summary([loss_summary20, accuracy20])\n",
    "}\n",
    "train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    " \n",
    "current_val_loss = tf.placeholder(tf.float32, name='validation_loss')\n",
    "current_val_acc = tf.placeholder(tf.float32, name='validation_acc')\n",
    "loss_summary = tf.scalar_summary(\"maximum-likelihood loss\", current_val_loss)\n",
    "accuracy_summary = accuracy10 = tf.scalar_summary(\"label accuracy\", current_val_acc)\n",
    "    \n",
    "# Dev summaries\n",
    "dev_summary_op = tf.merge_summary([loss_summary, accuracy_summary])\n",
    "dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "temp_stdout = sys.stdout\n",
    "sys.stdout = open('/dev/stdout', 'w')\n",
    "\n",
    "# define the iterative training steps here\n",
    "report_every = 50\n",
    "evaluate_every = 1000\n",
    "checkpoint_every = 10000\n",
    "\n",
    "num_epoch = 3\n",
    "\n",
    "checkpoint_path = 'checkpoints_sentence_extractor_1/stored_variables.ckpt'\n",
    "var_saver_2 = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "total_loss = 0.\n",
    "total_acc = 0. \n",
    "step_counter = 0.\n",
    "\n",
    "buckets = [10, 20]\n",
    "eval_buckets = [10, 20]\n",
    "\n",
    "is_sampled_weights = [0., 0., 0.]\n",
    "\n",
    "for epoch, r_weight in zip(range(num_epoch), is_sampled_weights):\n",
    "    r_elements = [True, False]\n",
    "    r_weights = [r_weight, 1.0 - r_weight]\n",
    "    sampled_number = 0\n",
    "    docs_trained = 0\n",
    "    val_global_counter = 0\n",
    "    for bucket in buckets:\n",
    "        while(bg.has_more(bucket, 'training')):\n",
    "            x_batch, y_batch = generate_batch(bucket, 5, 'training')\n",
    "\n",
    "            if len(x_batch) == 0:\n",
    "                continue # signaling if the batch is empty\n",
    "\n",
    "            # weighted random choice if sampled or not\n",
    "            sampled = np.random.choice(r_elements, p=r_weights)\n",
    "                \n",
    "            current_step, current_loss, current_acc = train_step(sess, x_batch, y_batch, feedforward_sampling=sampled)\n",
    "            total_acc += current_acc\n",
    "            total_loss += current_loss\n",
    "            step_counter += 1\n",
    "            docs_trained += len(x_batch)\n",
    "            if sampled:\n",
    "                sampled_number += len(x_batch)\n",
    "\n",
    "            if current_step % report_every == 0:\n",
    "                print(\"ep {}: bucket: {}, training step {}, loss avg {:g}, accuracy: {:g}, trained: {}, sampled: {}\".format(epoch, bucket, current_step, \n",
    "                                                                               total_loss / step_counter,\n",
    "                                                                               total_acc / step_counter, docs_trained, sampled_number))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                step_counter = 0\n",
    "\n",
    "            if current_step % evaluate_every == 0:\n",
    "                eval_loss = 0.\n",
    "                eval_acc = 0. \n",
    "                eval_counter = 0.\n",
    "                for eval_bucket in eval_buckets:\n",
    "                    print 'Evaluation on validation data bucket {0}:'.format(eval_bucket)\n",
    "                    while(bg.has_more(eval_bucket, 'validation')):\n",
    "                        x_val_batch, y_val_batch = generate_batch(eval_bucket, 5, 'validation')\n",
    "                        \n",
    "                        if len(x_val_batch) == 0:\n",
    "                            continue # signaling if the batch is empty\n",
    "                        \n",
    "                        val_loss, val_acc = eval_test_step(sess, x_val_batch, y_val_batch)\n",
    "                        eval_loss += val_loss\n",
    "                        eval_acc += val_acc\n",
    "                        eval_counter += 1\n",
    "                print(\"validation loss avg {:g}, accuracy: {:g}\".format(eval_loss / eval_counter,\n",
    "                                                                        eval_acc / eval_counter))\n",
    "                \n",
    "                val_summaries = sess.run(dev_summary_op, feed_dict={\n",
    "                        current_val_loss: eval_loss / eval_counter,\n",
    "                        current_val_acc: eval_acc / eval_counter\n",
    "                    })\n",
    "                dev_summary_writer.add_summary(val_summaries, current_step)\n",
    "                dev_summary_writer.flush()\n",
    "                \n",
    "                bg.reset_indices('validation')\n",
    "\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                ckpt_path = var_saver_2.save(sess, checkpoint_path+'.epoch'+str(epoch)+'.'+str(current_step))\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(ckpt_path))\n",
    "    \n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: epoch {} completed. docs trained: {}. sampled: {}\".format(time_str, epoch,\n",
    "                                                                            docs_trained, sampled_number))\n",
    "    \n",
    "    bg.reset_indices('training')\n",
    "        \n",
    "sys.stdout = temp_stdout\n",
    "print '\\nOptimization completed!\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# is_sampled_weights = [0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "# for epoch, weight in zip(range(num_epoch), is_sampled_weights):\n",
    "#     elements = [True, False]\n",
    "#     weights = [weight, 1.0 - weight]\n",
    "#     counter1 = 0\n",
    "#     counter2 = 0\n",
    "#     for i in range(10000):\n",
    "#         rand = np.random.choice(elements, p=weights)\n",
    "#         if rand:\n",
    "#             counter1 += 1\n",
    "#         else:\n",
    "#             counter2 += 1\n",
    "#     print 'counter1: {}'.format(counter1)\n",
    "#     print 'counter2: {}'.format(counter2)\n",
    "# y_batch = np.zeros((len(x_batch), 20))\n",
    "# input_dict = {placeholders['bucket_{0}'.format(20)]: x_batch,\n",
    "#               placeholders['sentence_labels_{0}'.format(20)]: y_batch,\n",
    "#               placeholders[\"feedfw_sampling\"]: True,\n",
    "#               placeholders[\"keep_prob\"]: 1.0}\n",
    "\n",
    "# res = sess.run(bucket20outputs, feed_dict=input_dict)\n",
    "# output_labels = np.transpose(np.squeeze(np.array(res)))\n",
    "# if len(x_batch) > 0:\n",
    "#     input_dict = {placeholders['bucket_20']: x_batch, \n",
    "#                      placeholders['sentence_labels_20']: y_batch,\n",
    "#                      placeholders[\"feedfw_sampling\"]: True,\n",
    "#                      placeholders[\"keep_prob\"]: 1.0}\n",
    "\n",
    "#     outs, rounds, tars, acc = sess.run([bucket20outputs, outputs20rounded, bucket20targets, accuracy20], feed_dict=input_dict)\n",
    "#     for out, ro, tar in zip(outs, rounds, tars):\n",
    "#         print str(out[0]) + '->' + str(ro[0]) + '->' + str(tar[0])\n",
    "#     print acc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import summary as sm\n",
    "for bucket in buckets:\n",
    "    while bg.has_more(bucket, 'test'):\n",
    "        test_batch, _, batch_filenames = generate_batch(bucket, 10, 'test', include_filenames=True)\n",
    "        \n",
    "        if len(test_batch) <= 1:\n",
    "                continue # signaling if the batch is empty\n",
    "        \n",
    "        y_filler = np.zeros((len(test_batch), bucket))\n",
    "        input_dict = {placeholders['bucket_{0}'.format(bucket)]: test_batch,\n",
    "              placeholders['sentence_labels_{0}'.format(bucket)]: y_filler,\n",
    "              placeholders[\"feedfw_sampling\"]: True,\n",
    "              placeholders[\"keep_prob\"]: 1.0}\n",
    "\n",
    "        out_probs = sess.run(bucket_outputs['bucket_{}'.format(bucket)], feed_dict=input_dict)\n",
    "        \n",
    "        out_probs = np.transpose(np.squeeze(np.array(out_probs)))\n",
    "\n",
    "        for i in range(len(out_probs)):\n",
    "            write_path = 'se_test_results_1/' + batch_filenames[i].split('/')[-1] + '.pred'\n",
    "            res = sm.get_summary(batch_filenames[i], out_probs[i], True)\n",
    "            with open(write_path, 'w') as f:\n",
    "                f.write(res)\n",
    "        \n",
    "bg.reset_indices('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
